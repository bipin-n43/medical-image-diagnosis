{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2EB-XE8vdYth"
   },
   "source": [
    "\n",
    "### **1. Data Cleaning**\n",
    "\n",
    "\n",
    "**1. Importing Libraries**\n",
    "The code starts by importing necessary libraries for the tasks:\n",
    "\n",
    "**PySpark:** Used for distributed data processing. It handles DataFrame manipulation and UDF registration.\n",
    "\n",
    "**NumPy:** Handles numerical operations, particularly for decoding images.\n",
    "\n",
    "**Matplotlib:** Used for visualizing data through plots.\n",
    "\n",
    "**OpenCV:** Decodes image bytes into NumPy arrays.\n",
    "\n",
    "**PyTorch:** Determines GPU availability and enhances computation using CUDA or MPS.\n",
    "\n",
    "**OS:** Manages file paths across operating systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oOhd0gEZjH8R"
   },
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, udf, sum as _sum\n",
    "from pyspark.sql.types import ArrayType, IntegerType\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import torch\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "phg0oPdneW4-"
   },
   "source": [
    "**2. Constants Definition**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "smEY18i1jJOk"
   },
   "outputs": [],
   "source": [
    "# Constants\n",
    "BASE_DIR = r\"/Users/ivantravisany/Documents/Estudios/git/medical-image-diagnosis-with-cnn/dataset/\"\n",
    "TRAIN_FILE = \"train-00000-of-00001-c08a401c53fe5312.parquet\"\n",
    "TEST_FILE = \"test-00000-of-00001-44110b9df98c5585.parquet\"\n",
    "LABELS = {\n",
    "    0: \"Mild Demented\",\n",
    "    1: \"Moderate Demented\",\n",
    "    2: \"Non Demented\",\n",
    "    3: \"Very Mild Demented\",\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YloTYLu0ebmU"
   },
   "source": [
    "**3. Decoding Images**\n",
    "\n",
    "**The decode_image function:** Takes byte-encoded images and converts them into grayscale. Flattens the image into a 1D array for efficient storage. This function is registered as a PySpark UDF (decode_image_udf) for parallel processing on Spark DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "1-zR1oSEjKtF"
   },
   "outputs": [],
   "source": [
    "# Function to decode images from byte dictionaries\n",
    "def decode_image(image_bytes):\n",
    "    \"\"\"\n",
    "    Decodes an image stored as bytes to a grayscale NumPy array.\n",
    "\n",
    "    Args:\n",
    "        image_bytes (bytes): Encoded image data as bytes.\n",
    "\n",
    "    Returns:\n",
    "        list: Flattened grayscale image as a list.\n",
    "    \"\"\"\n",
    "    if image_bytes is None:\n",
    "        return None\n",
    "    nparr = np.frombuffer(image_bytes, np.uint8)\n",
    "    img = cv2.imdecode(nparr, cv2.IMREAD_GRAYSCALE)\n",
    "    return img.flatten().tolist() if img is not None else None\n",
    "\n",
    "# Register UDF for decoding images\n",
    "decode_image_udf = udf(decode_image, ArrayType(IntegerType()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PUxS8MkVeu_f"
   },
   "source": [
    "**4. Initializing Spark Session and GPU Optimization**\n",
    "\n",
    "**PyTorch is used to check for GPU availability:**\n",
    "\n",
    "**CUDA:** If available, computations will run on NVIDIA GPUs.\n",
    "\n",
    "**MPS:** Support for Apple Silicon devices.\n",
    "\n",
    "**CPU:** Used as a fallback if no GPUs are available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2wnCESRBjOzY",
    "outputId": "39160748-c725-472a-e920-e7e55a0549ef"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/09 20:08:27 WARN Utils: Your hostname, Ivans-MacBook-Pro-2.local resolves to a loopback address: 127.0.0.1; using 192.168.2.18 instead (on interface en0)\n",
      "24/12/09 20:08:27 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/12/09 20:08:27 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using mps device\n"
     ]
    }
   ],
   "source": [
    "# Initialize Spark Session with necessary configurations for local file paths\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Medical Image Diagnosis\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Enable GPU optimizations if available\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# Determine the best available device (CUDA, MPS, or CPU)\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b7OhdSgPe29T"
   },
   "source": [
    "**5. Preprocessing Datasets**\n",
    "\n",
    "**The preprocess_dataset function:** Reads a Parquet file into a Spark DataFrame. Decodes the image.bytes column using the UDF and stores the results in a new column img_arr. And displays a preview of the dataset before and after decoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "wduCkxDnjPWE"
   },
   "outputs": [],
   "source": [
    "# Load and preprocess the dataset\n",
    "def preprocess_dataset(file_path):\n",
    "    \"\"\"\n",
    "    Loads and preprocesses the dataset by decoding images and dropping unused columns.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Path to the Parquet file.\n",
    "\n",
    "    Returns:\n",
    "        pyspark.sql.DataFrame: Preprocessed DataFrame with decoded images.\n",
    "    \"\"\"\n",
    "    # Load dataset into a Spark DataFrame\n",
    "    df = spark.read.parquet(file_path)\n",
    "\n",
    "    # Show a sample of the data without decoding the images\n",
    "    print(\"Training data without decoding images:\")\n",
    "    df.limit(5).show()\n",
    "\n",
    "    # Decode the 'image' column\n",
    "    df = df.withColumn(\"img_arr\", decode_image_udf(col(\"image.bytes\")))\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pbt-yuzXjRvg",
    "outputId": "66c98fd7-dc1f-406a-d378-2ebb7a13b36f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data without decoding images:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|               image|label|\n",
      "+--------------------+-----+\n",
      "|{[FF D8 FF E0 00 ...|    2|\n",
      "|{[FF D8 FF E0 00 ...|    0|\n",
      "|{[FF D8 FF E0 00 ...|    3|\n",
      "|{[FF D8 FF E0 00 ...|    3|\n",
      "|{[FF D8 FF E0 00 ...|    2|\n",
      "+--------------------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OpenCV bindings requires \"numpy\" package.                           (0 + 1) / 1]\n",
      "Install it via command:\n",
      "    pip install numpy\n",
      "24/12/09 20:08:37 ERROR Executor: Exception in task 0.0 in stage 2.0 (TID 2)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/Users/ivantravisany/Library/Python/3.9/lib/python/site-packages/cv2/__init__.py\", line 17, in <module>\n",
      "    raise\n",
      "  File \"/Users/ivantravisany/Library/Python/3.9/lib/python/site-packages/numpy/__init__.py\", line 141, in <module>\n",
      "    from . import core\n",
      "  File \"/Users/ivantravisany/Library/Python/3.9/lib/python/site-packages/numpy/core/__init__.py\", line 52, in <module>\n",
      "    del os.environ[envkey]\n",
      "ImportError: \n",
      "\n",
      "IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE!\n",
      "\n",
      "Importing the numpy C-extensions failed. This error can happen for\n",
      "many reasons, often due to issues with your setup or how NumPy was\n",
      "installed.\n",
      "\n",
      "We have compiled some common reasons and troubleshooting tips at:\n",
      "\n",
      "    https://numpy.org/devdocs/user/troubleshooting-importerror.html\n",
      "\n",
      "Please note and check the following:\n",
      "\n",
      "  * The Python version is: Python3.9 from \"/Applications/Xcode.app/Contents/Developer/usr/bin/python3\"\n",
      "  * The NumPy version is: \"1.24.4\"\n",
      "\n",
      "and make sure that they are the versions you expect.\n",
      "Please carefully study the documentation linked above for further help.\n",
      "\n",
      "Original error was: dlopen(/Users/ivantravisany/Library/Python/3.9/lib/python/site-packages/numpy/core/_multiarray_umath.cpython-39-darwin.so, 0x0002): tried: '/Users/ivantravisany/Library/Python/3.9/lib/python/site-packages/numpy/core/_multiarray_umath.cpython-39-darwin.so' (mach-o file, but is an incompatible architecture (have 'arm64', need 'x86_64')), '/System/Volumes/Preboot/Cryptexes/OS/Users/ivantravisany/Library/Python/3.9/lib/python/site-packages/numpy/core/_multiarray_umath.cpython-39-darwin.so' (no such file), '/Users/ivantravisany/Library/Python/3.9/lib/python/site-packages/numpy/core/_multiarray_umath.cpython-39-darwin.so' (mach-o file, but is an incompatible architecture (have 'arm64', need 'x86_64'))\n",
      "\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:94)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:75)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "24/12/09 20:08:38 WARN TaskSetManager: Lost task 0.0 in stage 2.0 (TID 2) (192.168.2.18 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/Users/ivantravisany/Library/Python/3.9/lib/python/site-packages/cv2/__init__.py\", line 17, in <module>\n",
      "    raise\n",
      "  File \"/Users/ivantravisany/Library/Python/3.9/lib/python/site-packages/numpy/__init__.py\", line 141, in <module>\n",
      "    from . import core\n",
      "  File \"/Users/ivantravisany/Library/Python/3.9/lib/python/site-packages/numpy/core/__init__.py\", line 52, in <module>\n",
      "    del os.environ[envkey]\n",
      "ImportError: \n",
      "\n",
      "IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE!\n",
      "\n",
      "Importing the numpy C-extensions failed. This error can happen for\n",
      "many reasons, often due to issues with your setup or how NumPy was\n",
      "installed.\n",
      "\n",
      "We have compiled some common reasons and troubleshooting tips at:\n",
      "\n",
      "    https://numpy.org/devdocs/user/troubleshooting-importerror.html\n",
      "\n",
      "Please note and check the following:\n",
      "\n",
      "  * The Python version is: Python3.9 from \"/Applications/Xcode.app/Contents/Developer/usr/bin/python3\"\n",
      "  * The NumPy version is: \"1.24.4\"\n",
      "\n",
      "and make sure that they are the versions you expect.\n",
      "Please carefully study the documentation linked above for further help.\n",
      "\n",
      "Original error was: dlopen(/Users/ivantravisany/Library/Python/3.9/lib/python/site-packages/numpy/core/_multiarray_umath.cpython-39-darwin.so, 0x0002): tried: '/Users/ivantravisany/Library/Python/3.9/lib/python/site-packages/numpy/core/_multiarray_umath.cpython-39-darwin.so' (mach-o file, but is an incompatible architecture (have 'arm64', need 'x86_64')), '/System/Volumes/Preboot/Cryptexes/OS/Users/ivantravisany/Library/Python/3.9/lib/python/site-packages/numpy/core/_multiarray_umath.cpython-39-darwin.so' (no such file), '/Users/ivantravisany/Library/Python/3.9/lib/python/site-packages/numpy/core/_multiarray_umath.cpython-39-darwin.so' (mach-o file, but is an incompatible architecture (have 'arm64', need 'x86_64'))\n",
      "\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:94)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:75)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n",
      "24/12/09 20:08:38 ERROR TaskSetManager: Task 0 in stage 2.0 failed 1 times; aborting job\n"
     ]
    },
    {
     "ename": "PythonException",
     "evalue": "\n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/Users/ivantravisany/Library/Python/3.9/lib/python/site-packages/cv2/__init__.py\", line 17, in <module>\n    raise\n  File \"/Users/ivantravisany/Library/Python/3.9/lib/python/site-packages/numpy/__init__.py\", line 141, in <module>\n    from . import core\n  File \"/Users/ivantravisany/Library/Python/3.9/lib/python/site-packages/numpy/core/__init__.py\", line 52, in <module>\n    del os.environ[envkey]\nImportError: \n\nIMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE!\n\nImporting the numpy C-extensions failed. This error can happen for\nmany reasons, often due to issues with your setup or how NumPy was\ninstalled.\n\nWe have compiled some common reasons and troubleshooting tips at:\n\n    https://numpy.org/devdocs/user/troubleshooting-importerror.html\n\nPlease note and check the following:\n\n  * The Python version is: Python3.9 from \"/Applications/Xcode.app/Contents/Developer/usr/bin/python3\"\n  * The NumPy version is: \"1.24.4\"\n\nand make sure that they are the versions you expect.\nPlease carefully study the documentation linked above for further help.\n\nOriginal error was: dlopen(/Users/ivantravisany/Library/Python/3.9/lib/python/site-packages/numpy/core/_multiarray_umath.cpython-39-darwin.so, 0x0002): tried: '/Users/ivantravisany/Library/Python/3.9/lib/python/site-packages/numpy/core/_multiarray_umath.cpython-39-darwin.so' (mach-o file, but is an incompatible architecture (have 'arm64', need 'x86_64')), '/System/Volumes/Preboot/Cryptexes/OS/Users/ivantravisany/Library/Python/3.9/lib/python/site-packages/numpy/core/_multiarray_umath.cpython-39-darwin.so' (no such file), '/Users/ivantravisany/Library/Python/3.9/lib/python/site-packages/numpy/core/_multiarray_umath.cpython-39-darwin.so' (mach-o file, but is an incompatible architecture (have 'arm64', need 'x86_64'))\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPythonException\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m train_df \u001b[38;5;241m=\u001b[39m preprocess_dataset(train_file_path)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Take a small sample of the data without triggering memory issues\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m sample_data \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlabel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mimg_arr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhead\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Display the sample data\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSample Training Data after decoding images:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pyspark/sql/dataframe.py:2969\u001b[0m, in \u001b[0;36mDataFrame.head\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m   2967\u001b[0m     rs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m   2968\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m rs[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m rs \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 2969\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtake\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pyspark/sql/dataframe.py:1401\u001b[0m, in \u001b[0;36mDataFrame.take\u001b[0;34m(self, num)\u001b[0m\n\u001b[1;32m   1372\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtake\u001b[39m(\u001b[38;5;28mself\u001b[39m, num: \u001b[38;5;28mint\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Row]:\n\u001b[1;32m   1373\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Returns the first ``num`` rows as a :class:`list` of :class:`Row`.\u001b[39;00m\n\u001b[1;32m   1374\u001b[0m \n\u001b[1;32m   1375\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1399\u001b[0m \u001b[38;5;124;03m    [Row(age=14, name='Tom'), Row(age=23, name='Alice')]\u001b[39;00m\n\u001b[1;32m   1400\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1401\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlimit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pyspark/sql/dataframe.py:1257\u001b[0m, in \u001b[0;36mDataFrame.collect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1237\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns all the records as a list of :class:`Row`.\u001b[39;00m\n\u001b[1;32m   1238\u001b[0m \n\u001b[1;32m   1239\u001b[0m \u001b[38;5;124;03m.. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1254\u001b[0m \u001b[38;5;124;03m[Row(age=14, name='Tom'), Row(age=23, name='Alice'), Row(age=16, name='Bob')]\u001b[39;00m\n\u001b[1;32m   1255\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1256\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SCCallSiteSync(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sc):\n\u001b[0;32m-> 1257\u001b[0m     sock_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollectToPython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1258\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, BatchedSerializer(CPickleSerializer())))\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mPythonException\u001b[0m: \n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/Users/ivantravisany/Library/Python/3.9/lib/python/site-packages/cv2/__init__.py\", line 17, in <module>\n    raise\n  File \"/Users/ivantravisany/Library/Python/3.9/lib/python/site-packages/numpy/__init__.py\", line 141, in <module>\n    from . import core\n  File \"/Users/ivantravisany/Library/Python/3.9/lib/python/site-packages/numpy/core/__init__.py\", line 52, in <module>\n    del os.environ[envkey]\nImportError: \n\nIMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE!\n\nImporting the numpy C-extensions failed. This error can happen for\nmany reasons, often due to issues with your setup or how NumPy was\ninstalled.\n\nWe have compiled some common reasons and troubleshooting tips at:\n\n    https://numpy.org/devdocs/user/troubleshooting-importerror.html\n\nPlease note and check the following:\n\n  * The Python version is: Python3.9 from \"/Applications/Xcode.app/Contents/Developer/usr/bin/python3\"\n  * The NumPy version is: \"1.24.4\"\n\nand make sure that they are the versions you expect.\nPlease carefully study the documentation linked above for further help.\n\nOriginal error was: dlopen(/Users/ivantravisany/Library/Python/3.9/lib/python/site-packages/numpy/core/_multiarray_umath.cpython-39-darwin.so, 0x0002): tried: '/Users/ivantravisany/Library/Python/3.9/lib/python/site-packages/numpy/core/_multiarray_umath.cpython-39-darwin.so' (mach-o file, but is an incompatible architecture (have 'arm64', need 'x86_64')), '/System/Volumes/Preboot/Cryptexes/OS/Users/ivantravisany/Library/Python/3.9/lib/python/site-packages/numpy/core/_multiarray_umath.cpython-39-darwin.so' (no such file), '/Users/ivantravisany/Library/Python/3.9/lib/python/site-packages/numpy/core/_multiarray_umath.cpython-39-darwin.so' (mach-o file, but is an incompatible architecture (have 'arm64', need 'x86_64'))\n\n"
     ]
    }
   ],
   "source": [
    "# Fix the file path by joining the BASE_DIR and TRAIN_FILE correctly\n",
    "train_file_path = os.path.join(BASE_DIR, TRAIN_FILE).replace(\"\\\\\", \"/\")\n",
    "\n",
    "# Preprocess the training dataset\n",
    "train_df = preprocess_dataset(train_file_path)\n",
    "\n",
    "# Take a small sample of the data without triggering memory issues\n",
    "sample_data = train_df.select(\"label\", \"img_arr\").head(5)\n",
    "\n",
    "# Display the sample data\n",
    "print(\"Sample Training Data after decoding images:\")\n",
    "for row in sample_data:\n",
    "    label = row['label']\n",
    "    img_arr = row['img_arr']  # Full image array, limiting the display size\n",
    "    print(f\"Label: {LABELS.get(label, 'Unknown')}, Image Array (first 10 pixels): {img_arr[:10]}\")  # First 10 pixels of the image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DU85lwmJfX39"
   },
   "source": [
    "**7. Training and Test Dataset Handling**\n",
    "\n",
    "The training dataset is preprocessed, and decoded images are displayed with their corresponding labels. The raw image column is dropped after processing to save memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2BDJnxH9ncuL"
   },
   "outputs": [],
   "source": [
    "# Define the function to decode images from byte dictionaries (dict_to_image)\n",
    "def dict_to_image(image_bytes):\n",
    "    \"\"\"\n",
    "    Decodes an image stored as bytes to a grayscale NumPy array.\n",
    "\n",
    "    Args:\n",
    "        image_bytes (bytes): Encoded image data as bytes.\n",
    "\n",
    "    Returns:\n",
    "        list: Flattened grayscale image as a list.\n",
    "    \"\"\"\n",
    "    if image_bytes is None:\n",
    "        return None\n",
    "    nparr = np.frombuffer(image_bytes, np.uint8)\n",
    "    img = cv2.imdecode(nparr, cv2.IMREAD_GRAYSCALE)\n",
    "    return img.flatten().tolist() if img is not None else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5of8YvuGXxsv",
    "outputId": "6a916fce-ea8f-46ee-d591-9c6fc4ec670d"
   },
   "outputs": [],
   "source": [
    "# Register UDF for decoding images\n",
    "dict_to_image_udf = udf(dict_to_image, ArrayType(IntegerType()))\n",
    "\n",
    "# Load the test dataset into a PySpark DataFrame\n",
    "train_df = spark.read.parquet(train_file_path)\n",
    "\n",
    "\n",
    "# Apply the dict_to_image function using the UDF and create a new column \"img_arr\"\n",
    "train_df = train_df.withColumn(\"img_arr\", dict_to_image_udf(col(\"image.bytes\")))\n",
    "\n",
    "# Drop the \"image\" column after transformation\n",
    "train_df = train_df.drop(\"image\")\n",
    "\n",
    "# Show the resulting DataFrame\n",
    "print(\"Training data after decoding images and dropping 'image' column:\")\n",
    "train_df.limit(5).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "me0YX0fymfHX",
    "outputId": "bca4f5ff-ebb4-4fd1-99df-cb7ead0a1cb2"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Fix the file path by joining the BASE_DIR and TEST_FILE correctly\n",
    "test_file_path = os.path.join(BASE_DIR, TEST_FILE).replace(\"\\\\\", \"/\")\n",
    "\n",
    "# Preprocess the training dataset\n",
    "test_df = preprocess_dataset(test_file_path)\n",
    "\n",
    "# Take a small sample of the data without triggering memory issues\n",
    "sample_data_test = test_df.select(\"label\", \"img_arr\").head(5)\n",
    "\n",
    "# Display the sample data\n",
    "print(\"Sample Testing Data after decoding images:\")\n",
    "for row in sample_data_test:\n",
    "    label = row['label']\n",
    "    img_arr = row['img_arr']  # Full image array, limiting the display size\n",
    "    print(f\"Label: {LABELS.get(label, 'Unknown')}, Image Array (first 10 pixels): {img_arr[:10]}\")  # First 10 pixels of the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DXCwpW02m_QX",
    "outputId": "003e29e8-dba3-4466-b5a5-cdc7f67f1a4e"
   },
   "outputs": [],
   "source": [
    "# Register UDF for decoding images\n",
    "dict_to_image_udf = udf(dict_to_image, ArrayType(IntegerType()))\n",
    "\n",
    "# Load the test dataset into a PySpark DataFrame\n",
    "test_df = spark.read.parquet(test_file_path)\n",
    "\n",
    "# Apply the dict_to_image function using the UDF and create a new column \"img_arr\"\n",
    "test_df = test_df.withColumn(\"img_arr\", dict_to_image_udf(col(\"image.bytes\")))\n",
    "\n",
    "# Drop the \"image\" column after transformation\n",
    "test_df = test_df.drop(\"image\")\n",
    "\n",
    "# Show the resulting DataFrame\n",
    "print(\"Test data after decoding images and dropping 'image' column:\")\n",
    "test_df.limit(5).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZeAQ5lk0fsHW"
   },
   "source": [
    "**8. Checking for Missing Values**\n",
    "\n",
    "**The check_missing_values function:** Computes the number of missing values for each column in the DataFrame. And helps identifying potential data quality issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vtIg6M9PWXOC"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def check_missing_values(df):\n",
    "    \"\"\"\n",
    "    Checks for missing (null) values in each column of the DataFrame.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): The PySpark DataFrame to check.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: A DataFrame with the count of missing values in each column.\n",
    "    \"\"\"\n",
    "    # Create a dictionary to store the missing values count for each column\n",
    "    missing_values = df.select([\n",
    "        _sum(col(c).isNull().cast(\"int\")).alias(c) for c in df.columns\n",
    "    ])\n",
    "    return missing_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aa-QBMmmXCW8",
    "outputId": "fde1fde7-f1a4-483b-f52e-fc36349ca77b"
   },
   "outputs": [],
   "source": [
    "# Check for missing values in train_df\n",
    "train_missing_values = check_missing_values(train_df)\n",
    "train_missing_values.show()\n",
    "\n",
    "# Check for missing values in test_df\n",
    "test_missing_values = check_missing_values(test_df)\n",
    "test_missing_values.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "spvIeI1iglHs"
   },
   "source": [
    "**9. Checking for Duplicates**\n",
    "\n",
    "**The check_duplicates function:** Compares the total row count with the count of unique rows. And identifies duplicate rows in the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eGoMxgJ5VArf"
   },
   "outputs": [],
   "source": [
    "# Function to check for duplicates in a DataFrame\n",
    "def check_duplicates(df):\n",
    "    \"\"\"\n",
    "    Checks for duplicate rows in the DataFrame.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): The PySpark DataFrame to check.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing the total number of rows and the number of duplicate rows.\n",
    "    \"\"\"\n",
    "    num_rows = df.count()\n",
    "    num_rows_no_duplicates = df.dropDuplicates().count()\n",
    "\n",
    "    return num_rows, num_rows_no_duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aOmXljFyWfML",
    "outputId": "7793d472-6564-449a-f69b-a06d54a98ad1"
   },
   "outputs": [],
   "source": [
    "# Check for duplicates in train_df\n",
    "train_num_rows, train_num_rows_no_duplicates = check_duplicates(train_df)\n",
    "\n",
    "if train_num_rows != train_num_rows_no_duplicates:\n",
    "    print(f\"There are {train_num_rows - train_num_rows_no_duplicates} duplicate rows in the training data.\")\n",
    "else:\n",
    "    print(\"No duplicate rows in the training data.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NjunZAhLW2UK",
    "outputId": "7d64a86f-f4bd-4009-c50d-747565040003"
   },
   "outputs": [],
   "source": [
    "# Check for duplicates in test_df\n",
    "test_num_rows, test_num_rows_no_duplicates = check_duplicates(test_df)\n",
    "\n",
    "if test_num_rows != test_num_rows_no_duplicates:\n",
    "    print(f\"There are {test_num_rows - test_num_rows_no_duplicates} duplicate rows in the test data.\")\n",
    "else:\n",
    "    print(\"No duplicate rows in the test data.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KKkmX_9DguaP"
   },
   "source": [
    "**10. Sampling and Visualizing Images**\n",
    "\n",
    "**A sample of the train decoded images:** Displayed in a 2x3 grid with their corresponding labels using Matplotlib. Assumes images are reshaped into 128x128 dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 303
    },
    "id": "vp-1E_iJoHQ1",
    "outputId": "b6716588-5469-4f05-c19d-fbf704e9994b"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Sample the data to avoid memory issues (taking 6 random images and their labels)\n",
    "sample_data = train_df.select(\"label\", \"img_arr\").limit(6).collect()\n",
    "\n",
    "# Convert the sample data to a format suitable for plotting\n",
    "fig, ax = plt.subplots(2, 3, figsize=(15, 5))\n",
    "axs = ax.flatten()\n",
    "\n",
    "# Loop through the axes and display images\n",
    "for i, axes in enumerate(axs):\n",
    "    # Get a random image and label from the sample\n",
    "    row = sample_data[i]\n",
    "    img_arr = np.array(row['img_arr']).reshape(128, 128)  # Assuming images are 128x128\n",
    "    label = row['label']\n",
    "\n",
    "    # Plot the image\n",
    "    axes.imshow(img_arr, cmap=\"gray\")\n",
    "    axes.set_title(LABELS.get(label, \"Unknown\"))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vingGOL1g91M"
   },
   "source": [
    "**11. Visualizing Label Distribution**\n",
    "\n",
    "The label distribution is plotted using a bar chart. Color-coded bars correspond to the categories in the LABELS dictionary.\n",
    "Provides insight into the dataset's class balance, as it is shown the dataset is imbalanced, which can lead to biased models that perform poorly on underrepresented classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 401
    },
    "id": "qFwSczN9ob7d",
    "outputId": "71e486ed-9936-4bd9-b618-703fdba9e5f9"
   },
   "outputs": [],
   "source": [
    "# Count the occurrences of each label in the 'label' column using Spark\n",
    "label_counts = train_df.groupBy(\"label\").count().orderBy(\"label\")\n",
    "\n",
    "# Convert the Spark DataFrame to Pandas for easy plotting\n",
    "label_counts_pd = label_counts.toPandas()\n",
    "\n",
    "# Define a color map for each label\n",
    "label_colors = {\n",
    "    0: 'skyblue',        # Mild Demented\n",
    "    1: 'lightgreen',     # Moderate Demented\n",
    "    2: 'lightcoral',     # Non Demented\n",
    "    3: 'lightgoldenrodyellow'  # Very Mild Demented\n",
    "}\n",
    "\n",
    "# Plotting the bar chart with colors for each label\n",
    "plt.figure(figsize=(9, 5))\n",
    "bars = plt.bar(label_counts_pd['label'], label_counts_pd['count'],\n",
    "               color=[label_colors[label] for label in label_counts_pd['label']])\n",
    "\n",
    "# Add totals above each bar\n",
    "for bar, count in zip(bars, label_counts_pd['count']):\n",
    "    plt.text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 5,\n",
    "             f\"{count}\", ha='center', va='bottom')\n",
    "\n",
    "# Customize the plot\n",
    "plt.ylabel(\"Number of Images\")\n",
    "plt.xticks(np.arange(0, 4, 1), labels=[LABELS[i] for i in range(4)])\n",
    "plt.title(\"Distribution of Labels with Totals\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OrI1T_YocRzg"
   },
   "source": [
    "# **2.Feature Extraction and Visualization**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yFKZkq8jciBK"
   },
   "source": [
    "### **1. Feature Extraction:**\n",
    "The extract_features function creates new columns for statistical features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xRs_5Q49fN5c",
    "outputId": "010a9a56-870a-4e3c-a1e3-56b70abea048"
   },
   "outputs": [],
   "source": [
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tx940oYVfdCN"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import mean, stddev, max, min, col, expr\n",
    "from pyspark.ml.stat import Correlation\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Feature Extraction\n",
    "def extract_features(df):\n",
    "    # Check if 'features' column already exists\n",
    "    if 'features' not in df.columns:  # Only add features if it doesn't exist\n",
    "        # Extract simple statistical features from images\n",
    "        df = df.withColumn(\"mean\", expr(\"aggregate(img_arr, 0D, (acc, x) -> acc + x) / size(img_arr)\"))  # Calculate mean using aggregate function\n",
    "        df = df.withColumn(\"std\", expr(\"sqrt(aggregate(img_arr, 0D, (acc, x) -> acc + (x - mean) * (x - mean)) / (size(img_arr) - 1))\").alias(\"std\"))  # Calculate standard deviation\n",
    "        df = df.withColumn(\"max\", expr(\"array_max(img_arr)\"))  # Calculate max\n",
    "        df = df.withColumn(\"min\", expr(\"array_min(img_arr)\"))  # Calculate min\n",
    "\n",
    "        # Assemble features into a vector\n",
    "        assembler = VectorAssembler(inputCols=[\"mean\", \"std\", \"max\", \"min\"], outputCol=\"features\")\n",
    "        df = assembler.transform(df)\n",
    "    return df # Return the DataFrame, modified or not"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3kNZFvhEdbhC"
   },
   "source": [
    "This function calculates the mean, standard deviation, maximum, and minimum values of each image's pixel array. It then assembles these features into a vector column named \"features\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a-ZSsc-ddg0f"
   },
   "source": [
    "### **2. Apply Feature Extraction:**\n",
    "\n",
    "The function is applied to both training and test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5EDBEu7Jdl5x"
   },
   "outputs": [],
   "source": [
    "# Apply feature extraction\n",
    "train_df = extract_features(train_df)\n",
    "test_df = extract_features(test_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TdpIOUbBe4hn"
   },
   "source": [
    "### **3. Feature Correlation Visualization:**\n",
    "This code creates a correlation matrix of the extracted features and visualizes it using a heatmap. This helps identify relationships between different statistical properties of the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 719
    },
    "id": "m3tt4_2afnyZ",
    "outputId": "9e89770f-529b-46dd-dca9-4a6c250c7d62"
   },
   "outputs": [],
   "source": [
    "feature_cols = [\"mean\", \"std\", \"max\", \"min\"]\n",
    "# Select the 'features' column along with the other feature columns\n",
    "feature_df = train_df.select(\"features\", *feature_cols)\n",
    "correlation_matrix = Correlation.corr(feature_df, \"features\").collect()[0][0].toArray()\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap=\"coolwarm\", xticklabels=feature_cols, yticklabels=feature_cols)\n",
    "plt.title(\"Feature Correlation Heatmap\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oMfjwac3gG1A"
   },
   "source": [
    "### **4. Feature Distribution Visualization:**\n",
    "For each extracted feature (mean, std, max, min), it creates a histogram with a kernel density estimate (KDE) curve to show the distribution shape. This helps understand the distribution of each feature across the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "hrEB5yNdgS2U",
    "outputId": "7df354cd-0e83-46df-a91f-1c47a6183f1a"
   },
   "outputs": [],
   "source": [
    "for feature in feature_cols:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.histplot(data=train_df.select(feature).toPandas(), x=feature, kde=True)\n",
    "    plt.title(f\"Distribution of {feature}\")\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
